---
title: "Alternative R Code to Statistical Rethinking: Chapter 3"
author: "Adam Loy, Math 315"
date: "Fall 2018"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(rethinking)
library(dplyr)
library(ggformula)
knitr::opts_chunk$set(exercise.timelimit = 60)
```

## Overview

This web app provides alternative code for each code chunk presented in *Statistical Rethinking* by Richard McElreath. Many of the chunks have been adapted from Randy Pruim's [github repo](https://github.com/rpruim/Statistical-Rethinking). The main idea is to present tidier code with preference for

 * related data be in containers (lists, data frames, etc.) rather than in "loose vectors".
 * not using the same names over and over for different things.
 * using `ggplot2` for graphics.

## Basic calculations

#### R code 3.1

There is no need for alternative code here. 

```{r chunk3.1}
PrPV <- 0.95
PrPM <- 0.01
PrV <- 0.001
PrP <- PrPV * PrV + PrPM * (1 - PrV)
(PrVP <- PrPV * PrV / PrP)
```

## 3.1 Sampling from a grid-approximate posterior


To begin, we can define a new function, `densify()`, that is useful for rescaling prior and posterior distributions when using the grid-approximation method. It takes the kernel of a gridded (i.e. *rasterized*) distribution and rescales it to be a proper density.

```{r densify}
# this function turns a "rasterized" kernel into a true (rasterized) density function
# useful for rescaling priors and posteriors when using the grid method

densify <- 
  function(x, y) {
    if ( diff(range(diff(x))) > 0.1 * min(abs(diff(x))) )
      stop("`x` must be (approximately) equally spaced.")
    
    width <- mean(diff(x))
    y / sum(y) / width
  }
```


#### R code 3.2

Here, we again use `mutate()` to avoid repeatedly using the same object names. This code chunk is computing the posterior via grid approximation, where the grid for `p` is 1000 values between 0 and 1.

```{r chunk3.2a}
Water9 <- 
  data_frame(p = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(
    prior = 1,
    likelihood = dbinom(6, size = 9, prob = p),
    posterior = likelihood * prior,
    posterior = posterior / sum(posterior),
    posterior.dens = densify(p, posterior)
  )
```

#### R code 3.3

You book only stores the sampled values of $p$ from the posterior as a vector (`water9.ps.p`); however, if you use `ggplot2` for plotting it is useful to have them organized as a data frame (`water9.ps.df`).

```{r chunk3.3a}
# vector
water9.ps.p <- with(Water9, sample(p, prob = posterior, size = 1e4, replace = TRUE))

# data frame
water9.ps.df <- data_frame(index = 1:length(water9.ps.p), p = water9.ps.p)
```

#### R code 3.4 and 3.5

Here, we recreate the two panels of Figure 3.1 using 1 `ggplot2`.

```{r, chunk3.4a}
# Chunk 3.4
ggplot(water9.ps.df, aes(x = index, y = p)) +
  geom_point() +
  labs(x = "index", y = "p")
```

```{r chunk3.5a}
# Chunk 3.5
ggplot(water9.ps.df) +
  geom_density(aes(x = p), color = NA, fill = "gray60", alpha = 0.5)
```

We could also add the rasterized posterior distribution enabling comparison between the distribution of the samples from the posterior and the distribution itself.

```{r chunk3.5b}
# Add the posterior for reference
ggplot(water9.ps.df) +
  geom_density(aes(x = p), color = NA, fill = "gray60", alpha = 0.5) +
  geom_line(data = Water9, aes(x = p, y = posterior.dens), color = "red")
```



## 3.2 Sampling to summarize

#### R code 3.6

There are multiple ways to add up the posterior probability where $p < 0.5$. Here are two reasonable approaches:

The book nests everything together. Remember that we stored the posterior in the `Water9` data frame.

```{r chunk3.6a}
sum(Water9$posterior[Water9$p < 0.5])
```

We could also fully embrace the chaining syntax and `dplyr` tools. Here we create two groups: one where $p < 0.5$; the other where $p \ge 0.5$. Next, the values of the posterior are summed within each group.

```{r chunk3.6b}
Water9 %>% 
  group_by(p < 0.5) %>% 
  summarize(sum(posterior))
```

#### R code 3.7

Approach to calculate the frequency (estimated probability) that the parameter value is below 0.5:

```{r, chunk3.7}
sum(water9.ps.p < 0.5) / length(water9.ps.p)
```


#### R code 3.8

You can also find the probability of intersections using `&`:

```{r, chunk3.8}
sum(water9.ps.p > 0.5 & water9.ps.p < 0.75) / length(water9.ps.p)
```

#### R code 3.9

Basic statistical functions are also important to summarize the posterior distribution:

```{r, chunk3.9a}
quantile(water9.ps.p, 0.8)
```

#### R code 3.10

```{r, chunk3.10a}
quantile(water9.ps.p, c(0.1, 0.9))
```

#### R code 3.11

Here, we calculate the posterior via grid approximation and then draw 1000 samples from it.

```{r, chunk3.11a}
Water3 <-
  expand.grid(p = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate(
    prior = 1,   # recycling makes this into 1000 1's
    prior = prior / sum(prior),
    likelihood = dbinom(3, size = 3, prob = p),
    posterior = likelihood * prior,
    posterior = posterior / sum(posterior),
    posterior.dens = densify(p, posterior)    # true density
  )

water3.ps.p <- with(Water3, sample(p, size = 1e4, replace = TRUE, prob = posterior))
```

#### R code 3.12

Percentile intervals can be calculated using the `PI()` function in the `rethinking` package. You could also use the `quantile()` function.

```{r chunk3.12a}
PI(water3.ps.p, prob = 0.5)
```

#### R code 3.13

Highest posterior density intervals are calculated using the `HPDI()` function in the `rethinking` package.

```{r chunk3.13a}
HPDI(water3.ps.p, prob = 0.5)
```

#### R code 3.14

If you are simply investigating what the *maximum a posteriori* (MAP) estimate of $p$ is, you can simply arrange your data frame in descending order by the posterior probability and print only the first row:


```{r chunk3.14a}
Water3 %>% 
  arrange(desc(posterior)) %>% #desc() for descending order
  head(1)
```

If you want to return the MAP estimate of $p$, then using indexing makes sense (similar to the code shown in the book):

```{r}
Water3$p[which.max(Water3$posterior)]
```


#### R code 3.15

The `chainmode()` function returns the estimated mode of a density computed from samples. The (optional) argument `adj` adjusts the amount of smoothing used to estimate the smoothed density. To better understand this, explore what happens when you use `adj = 1` (default amount of smoothing), `adj = 0.1` (less smoothing), and adj = `0.01` (lots less smoothing). Do the same for the `adjust` parameter in `gf_density()`.

```{r chunk3.15a, exercise = TRUE}
chainmode(water3.ps.p, adj = 1) 
gf_density( ~water3.ps.p, adjust = 1)
```

#### R code 3.16

No need for a change here.

```{r, chunk3.16}
mean(water3.ps.p)
median(water3.ps.p)
```

#### R code 3.17

We can use the `with()` function to avoid using the `$` operator to extract columns, but it isn't necessary. The below commands are equivalent:

```{r, chunk3.17}
with(Water3, sum(posterior * abs(0.5 - p)))
sum(Water3$posterior * abs(0.5 - Water3$p))
```

Another approach is to estimate the expected loss from the posterior samples. There will be small differences here due to sampling variability:

```{r, chunk3.17a}
mean(abs(water3.ps.p - 0.5))
```

#### R code 3.18

You book creates a new `loss` vector in the working space. Instead, you can add a `loss` column to the `Water3` data frame:

```{r, chunk3.18a}
Water3 <- 
  Water3 %>% 
  mutate(loss = sapply(p, function(d) {sum(posterior * abs(d - p))})
  )
```

#### R code 3.19

Again, we can either extract the minimum value or arrange the data frame to reveal the minimum value.

```{r, chunk3.19a}
# extract the minimum
with(Water3, p[which.min(loss)])

# a tidy alternative based on arranging the data frame
Water3 %>% 
  arrange(loss) %>% 
  head(1)
```


## 3.3 Sampling to simulate prediction

#### R code 3.20

No need for a change here.

```{r, chunk3.20}
dbinom(0:2, size = 2, prob = 0.7)
```


#### R code 3.21

No need for a change here.

```{r, chunk3.21}
rbinom(1, size = 2, prob = 0.7)
```

#### R code 3.22

No need for a change here.

```{r, chunk3.22}
rbinom(10, size = 2, prob = 0.7)
```

#### R code 3.23

No need for a change here.

```{r, chunk3.23}
dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w) / 1e5
```


#### R code 3.24

Here is `ggplot2` code to create a histogram for **discrete** random variables. For continuous random variables use `geom_histogram()` rather than `geom_bar()`.

```{r, chunk3.24}
data_frame(w = dummy_w) %>%
  ggplot(aes(x = w)) +
  geom_bar() +
  labs(x = "dummy water count")
```

#### R code 3.25

No need for a change here.

```{r chunk3.25}
w <- rbinom(1e4, size = 9, prob = 0.6)
```

#### R code 3.26

The only change here is that we named the vector of samples more meaningfully.

```{r chunk3.26a}
w <- rbinom(1e4, size = 9, prob = water9.ps.p)
```

#### R code 3.27

Again, let's avoid recycling vector names by storing the pieces in a data frame.

```{r chunk3.27a}
Water9a <-
  expand.grid(p = seq(from = 0, to = 1, length.out = 1000)) %>%
  mutate( 
    prior = 1,
    likelihood = dbinom(6, size = 9, prob = p),
    posterior = likelihood * prior,
    posterior = posterior / sum(posterior),
    posterior.dens = densify(p, posterior)
  )

set.seed(100)
sampled_p <- 
  with(Water9a, sample(p, prob = posterior, size = 1e4, replace = TRUE))
```

#### R code 3.28

No need for a change here, this simply reminds you that you can input vectors by hand.

```{r chunk3.28}
birth1 <- c(1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
            0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
            1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
            0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1)

birth2 <- c(0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
            0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
            0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0)
```

If you don't want to stare at 0s and 1s, you can use a more transparent coding scheme, such as F and M:

```{r chunk3.28a}
BirthSex <-
  data_frame(
    first = c("F", "M") [1 + birth1],
    second = c("F", "M")[1 + birth2]
  ) %>% 
  mutate(boys = (first == "M") + (second == "M"), girls = 2 - boys) 
head(BirthSex)
```

#### R code 3.29

No need for a change here, this simply loads the `birth1` and `birth2` vectors from the R package rather than using manual entry.

```{r chunk3.29}
library(rethinking)
data(homeworkch3)
```


#### R code 3.30

If you used the more transparent coding scheme, you can create cumulative sums by groups:

```{r chunk3.30a}
BirthSex %>% 
  group_by(boys) %>%
  summarise(n = n()) %>%
  mutate(cumsum(n),  total.boys = boys * n,  cumsum(total.boys))
```

